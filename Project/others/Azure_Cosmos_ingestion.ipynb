{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, exceptions\n",
    "\n",
    "import csv \n",
    "import json\n",
    "import sys\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from src.AzureCosmos_connector import COSMOS_CONNECTOR\n",
    "from src.config import azure_schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmosDB = COSMOS_CONNECTOR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmosDB.list_databases()\n",
    "print(\"=\"*10)\n",
    "cosmosDB.find_database(\"image-embeddings-db\")\n",
    "print(\"=\"*10)\n",
    "emd_db = cosmosDB.get_database(\"image-embeddings-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmosDB.list_containers(emd_db)\n",
    "print(\"=\"*10)\n",
    "cosmosDB.find_container(emd_db, \"bert-encodings\")\n",
    "print(\"=\"*10)\n",
    "bertContainer= cosmosDB.get_container(emd_db, \"bert-encodings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(bertContainer.query_items(\n",
    "        query=\"SELECT * FROM r WHERE r.id=@id\",\n",
    "        parameters=[\n",
    "            { \"name\":\"@id\", \"value\": \"images_000000000001\" }\n",
    "        ],\n",
    "        enable_cross_partition_query=True\n",
    "    ))\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dflist = []\n",
    "for item in bertContainer.query_items(\n",
    "    query='SELECT * FROM c',\n",
    "    enable_cross_partition_query = True):\n",
    "    dflist.append(dict(item))\n",
    "df = pd.DataFrame(dflist)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/azure_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import axis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['encd'].to_list()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = df.copy()\n",
    "column_names = [f\"encd_{i}\" for i in range(768)]\n",
    "feature_dataset[column_names] = pd.DataFrame(df['encd'].to_list(), columns=column_names)\n",
    "feature_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" PCA => K-Means: curse of Dimensionality \n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = feature_dataset[column_names].values\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, 701, step=20)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y[xi], marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, 701, step=20)) #change from 0-based array index to 1-based human-readable label\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()\n",
    "\n",
    "pca = PCA(0.95).fit(scaled_features)\n",
    "reduced_features = pca.transform(scaled_features)\n",
    "reduced_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" K-Means (Partitional Clustering)\n",
    "\"\"\"\n",
    "from kneed import KneeLocator\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "kmeans_kwargs = {\n",
    "    \"init\": \"random\",\n",
    "    \"n_init\": 10,\n",
    "    \"max_iter\": 300,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "sse = []\n",
    "k_samples = range(1, 103, 3)\n",
    "\n",
    "for k in k_samples:\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(reduced_features)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.plot(k_samples, sse)\n",
    "plt.xticks(k_samples)\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the elbow point of the curve\n",
    "kl = KneeLocator(k_samples, sse, curve=\"convex\", direction=\"decreasing\")\n",
    "print(kl.elbow)\n",
    "\n",
    "kmeans = KMeans(n_clusters=kl.elbow, **kmeans_kwargs)\n",
    "kmeans.fit(reduced_features)\n",
    "\n",
    "#%%\n",
    "# kmeans.cluster_centers_.shape\n",
    "def map_centroid(row):\n",
    "    return kmeans.cluster_centers_[row[\"kmeans_cluster_id\"]].tolist()\n",
    "# %%\n",
    "df[\"kmeans_cluster_id\"] = kmeans.labels_\n",
    "display(df.head(2))\n",
    "df[\"kmeans_cluster_centroid\"] = df.apply(map_centroid, axis=1)\n",
    "display(df.head(2))\n",
    "df['kmeans_cluster_id'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" store PCA & clustering model\n",
    "\"\"\"\n",
    "from joblib import dump, load\n",
    "# scaler\n",
    "dump(scaler, './models/full/scaler.joblib')\n",
    "dump(pca, './models/full/pca.joblib')\n",
    "dump(kmeans, './models/full/kmeans.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmosDB.list_containers(emd_db)\n",
    "print(\"=\"*10)\n",
    "cosmosDB.find_container(emd_db, \"clustered-meta-data\")\n",
    "print(\"=\"*10)\n",
    "bertContainer= cosmosDB.get_container(emd_db, \"clustered-meta-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_client.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database_client = client.get_database_client(\"image-embeddings-db\")\n",
    "container_client = emd_db.get_container_client(\"clustered-meta-data\")\n",
    "num = 1\n",
    "\n",
    "for row in df.to_dict('records'):\n",
    "    try:\n",
    "        container_client.upsert_item(\n",
    "            {\n",
    "                'id': row[\"id\"],\n",
    "                'cluster_id': row[\"kmeans_cluster_id\"],\n",
    "                'cluster_centroid':row[\"kmeans_cluster_centroid\"],\n",
    "                'dataset': row[\"dataset\"],\n",
    "                'url': row['url'],\n",
    "                'encd': row['encd'],\n",
    "                'img_name': row['img_name']\n",
    "            }\n",
    "        )      \n",
    "    except exceptions.CosmosHttpResponseError as e:\n",
    "        print(\"Failed to insert {}, row number {}\".format(row['id'], num))\n",
    "        print(e)\n",
    "    else:\n",
    "        print(\"Inserted {}, row number {}\".format(row['id'], num))\n",
    "    num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(tuple([\"images_000000000001\", \"images_000000000016\"]))\n",
    "# '\\',\\''.join(Ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n1.2 Reading Item by Id\\n')\n",
    "# doc_id = \"images_000000000001\"\n",
    "# # Note that Reads require a partition key to be spcified.\n",
    "# response = container_client.read_item(item=doc_id, partition_key=doc_id)\n",
    "Ids = [\"images_000000000001\", \"images_000000000016\"]\n",
    "Ids = str(tuple([\"images_000000000001\", \"images_000000000016\"]))\n",
    "items = list(container_client.query_items(\n",
    "            query=f\"SELECT * FROM r WHERE r.id IN {Ids}\",\n",
    "            # parameters=[\n",
    "            #     { \"name\":\"@ids\", \"value\": \"\\',\\'\".join(Ids) }\n",
    "            # ],\n",
    "            enable_cross_partition_query=True\n",
    "        ))\n",
    "pd.DataFrame(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(container_client.query_items(\n",
    "            query=\"SELECT * FROM r WHERE r.id IN (@ids)\",\n",
    "            parameters=[\n",
    "                { \"name\":\"@ids\", \"value\": ','.join(Ids) }\n",
    "            ],\n",
    "            enable_cross_partition_query=True\n",
    "        ))\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert(client: CosmosClient, csvFilePath):\n",
    "    database_client = client.get_database_client(\"image-embeddings-db\")\n",
    "    container_client = database_client.get_container_client(\"clustered-meta-data\")\n",
    "\n",
    "    #read csv file\n",
    "    with open(csvFilePath, encoding='utf-8') as csvf: \n",
    "        #load csv file data using csv library's dictionary reader\n",
    "        csvReader = csv.DictReader(csvf) \n",
    "\n",
    "        num = 1\n",
    "\n",
    "        #convert each csv row into python dict\n",
    "        for row in csvReader: \n",
    "            row[\"dataset\"] = \"images\"\n",
    "            # row[\"url\"] = \"https://cs5425images.blob.core.windows.net/test-images/{}\".format(row[\"img_name\"])\n",
    "            row[\"encd\"] = json.loads(row[\"encd\"])\n",
    "            row[\"id\"] = \"{}_{}\".format(row[\"dataset\"], row[\"img_name\"])\n",
    "\n",
    "            try:\n",
    "                container_client.upsert_item(\n",
    "                    {\n",
    "                        'id': row[\"id\"],\n",
    "                        'dataset': row[\"dataset\"],\n",
    "                        'url': row['url'],\n",
    "                        'encd': row['encd'],\n",
    "                        'img_name': row['img_name']\n",
    "                    }\n",
    "                )      \n",
    "            except exceptions.CosmosHttpResponseError as e:\n",
    "                print(\"Failed to insert {}, row number {}\".format(row['id'], num))\n",
    "                print(e)\n",
    "                return\n",
    "            else:\n",
    "                print(\"Inserted {}, row number {}\".format(row['id'], num))\n",
    "\n",
    "            num += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "920bfbcc0db8631c224b11aa06178e6cde32153641dabc78dcbceffb115d1df4"
  },
  "kernelspec": {
   "display_name": "CS5446",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
